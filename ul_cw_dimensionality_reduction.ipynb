{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zrshen/ML_Colab/blob/main/ul_cw_dimensionality_reduction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PaBH22wZq7K2"
      },
      "source": [
        "# **Dimensionality Reduction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKgFPLwWq7K4"
      },
      "source": [
        "# Setup ðŸ“‹"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fv10Ljgq7K5"
      },
      "source": [
        "# Python â‰¥3.5 is required\n",
        "import sys\n",
        "assert sys.version_info >= (3, 5)\n",
        "\n",
        "# Scikit-Learn â‰¥0.20 is required\n",
        "import sklearn\n",
        "assert sklearn.__version__ >= \"0.20\"\n",
        "\n",
        "# Common imports\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Filter warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# to make this notebook's output stable across runs\n",
        "np.random.seed(42)\n",
        "\n",
        "# To plot pretty figures\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)\n",
        "\n",
        "# Where to save the figures\n",
        "PROJECT_ROOT_DIR = \".\"\n",
        "CHAPTER_ID = \"dim_reduction\"\n",
        "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
        "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
        "\n",
        "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
        "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
        "    print(\"Saving figure\", fig_id)\n",
        "    if tight_layout:\n",
        "        plt.tight_layout()\n",
        "    plt.savefig(path, format=fig_extension, dpi=resolution)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmBlqrVrq7LP"
      },
      "source": [
        "# Exercises âœï¸"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0U7fwiQxq7LP"
      },
      "source": [
        "**Load the MNIST dataset and split it into a training set and a test set (take the first 5,000 instances for training, and the next 2,500 for testing).**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FEczQpTsVVn"
      },
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "\n",
        "# Load data and cast target into an numerical type\n",
        "mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
        "mnist.target = mnist.target.astype(np.uint8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kp7lxaRdq7LP"
      },
      "source": [
        "# Manually separate X and y into train and test\n",
        "X_train = mnist['data'][:5000]\n",
        "y_train = mnist['target'][:5000]\n",
        "\n",
        "X_test = mnist['data'][5000:7500]\n",
        "y_test = mnist['target'][5000:7500]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpguiIfUq7LP"
      },
      "source": [
        "**Train a Random Forest classifier on the dataset and time how long it takes, then evaluate the resulting model on the test set.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "poEjwYObq7LP"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Instantiate the model\n",
        "rnd_clf = RandomForestClassifier(n_estimators=100, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOjof_R5q7LP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46c4dfd4-9e81-446c-e652-8eb00091551c"
      },
      "source": [
        "import time\n",
        "\n",
        "# Time the model training\n",
        "t0 = time.time()\n",
        "rnd_clf.fit(X_train, y_train)\n",
        "t1 = time.time()\n",
        "print(\"Training took {:.2f}s\".format(t1 - t0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training took 2.91s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2EXc8yNq7LQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08e3660f-104d-4a93-b40a-ad9846c8556c"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Check accuracy score\n",
        "y_pred = rnd_clf.predict(X_test)\n",
        "accuracy_score(y_test, y_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9368"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcaO7Avkq7LQ"
      },
      "source": [
        "**Next, use PCA to reduce the dataset's dimensionality, with an explained variance ratio of 95%.**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Refresher: what is PCA? Principal Component Analysis is a technique to reduce the dimensionality of data. \n",
        "\n",
        "Conceptually, PCA extracts \"new\", more valuable features from existing data by mapping existing data a new space. The axes in this new space are along the lines of the most variance in the original space. Each of these new axes have an order of importance, and are orthogonal (uncorrelated) to each other. If you have 30-dimensional data, PCA will spit out 30 new axes. However, you can use fewer than 30, let's say half (15) to capture MORE than half of the variance in the data in a smaller (lower dimensional) format.\n",
        "\n",
        "Mathematically, PCA works by performing eigenvalue decomposition on the X (data) matrix. Each principal component (axis) is described by the eigenvector corresponding the largest eigen value, second largest, third largest, and so on.\n",
        "\n",
        "Strengths:\n",
        "- PCA can simplify your data dramatically, improving training and prediction time.\n",
        "\n",
        "Weaknesses:\n",
        "- Can provide a different result on the same data than a technique such as LDA. Be careful about which technique you use, since they optimize for different things.\n",
        "- Interpretation of results becomes impossible. Principle components are not features a person can conceptually map to real world interpretations in any practical way.\n",
        "- Can dampen predictive signals in the data"
      ],
      "metadata": {
        "id": "wh7oUQxGjTc8"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syrzMJvJq7LQ"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=0.95)\n",
        "X_train_reduced = pca.fit_transform(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRIHwiv2q7LQ"
      },
      "source": [
        "**Train a new Random Forest classifier on the reduced dataset and see how long it takes. Was training much faster?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGQGOIrAq7LQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "040e5dea-8c36-4cf9-e372-e793581fbc95"
      },
      "source": [
        "rnd_clf2 = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "t0 = time.time()\n",
        "rnd_clf2.fit(X_train_reduced, y_train)\n",
        "t1 = time.time()\n",
        "print(\"Training took {:.2f}s\".format(t1 - t0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training took 5.84s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2CU3QWnq7LQ"
      },
      "source": [
        "ðŸ›‘ Oh no! Training is actually more than twice slower now! How can that be? ðŸ›‘\n",
        "\n",
        "Dimensionality reduction does not always lead to faster training time: it depends on the dataset, the model and the training algorithm.\n",
        "\n",
        "If we try a softmax classifier instead of a random forest classifier, we may find that the training time is reduce when using PCA, but let's first check the accuracy of the random forest classifier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-Mxt8Nzq7LQ"
      },
      "source": [
        "**Next, evaluate the classifier on the test set: how does it compare to the previous classifier?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsCc6nU5q7LQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b9b9781-40f8-4130-d4b3-294a85c07a66"
      },
      "source": [
        "# Transform and predict on PCA data\n",
        "X_test_reduced = pca.transform(X_test)\n",
        "y_pred = rnd_clf2.predict(X_test_reduced)\n",
        "accuracy_score(y_test, y_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9036"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUx-zxpFq7LR"
      },
      "source": [
        "It is common for performance to drop slightly when reducing dimensionality, because we do lose some useful signal in the process. However, the performance drop was not too bad in this case. Sometimes, it can be a lot worse even with 95% of the explained variance ratio like we just used. With higher training time and lower accuracy, PCA really did not help: it slowed down training and reduced performance â˜¹ï¸\n",
        "\n",
        "Let's see if it helps when using softmax regression:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9tXNkyWq7LR"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "log_clf = LogisticRegression(multi_class=\"multinomial\", solver=\"lbfgs\", random_state=42)\n",
        "t0 = time.time()\n",
        "log_clf.fit(X_train, y_train)\n",
        "t1 = time.time()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mwW-qjHq7LR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "147ea3a2-6d72-4db5-ca5f-c5b3adbeb6b2"
      },
      "source": [
        "print(\"Training took {:.2f}s\".format(t1 - t0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training took 3.95s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKgO4WLIq7LR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d4da5d3-4efe-4dc0-c0f1-8e37887080d2"
      },
      "source": [
        "y_pred = log_clf.predict(X_test)\n",
        "accuracy_score(y_test, y_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8712"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05Jqy8qAq7LR"
      },
      "source": [
        "As seen, softmax regression takes much longer to train on this dataset than the random forest classifier, and it performs worse on the test set. But that's not what we are interested in right now; we want to see how much PCA can help softmax regression. Let's train the softmax regression model using the reduced dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GW6IEU0Oq7LR"
      },
      "source": [
        "# Instantiate model\n",
        "log_clf2 = LogisticRegression(multi_class=\"multinomial\", solver=\"lbfgs\", random_state=42)\n",
        "\n",
        "# Time training\n",
        "t0 = time.time()\n",
        "log_clf2.fit(X_train_reduced, y_train)\n",
        "t1 = time.time()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1CVlCuFq7LR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50d14688-9c82-4a2f-82c0-76d657e42d7c"
      },
      "source": [
        "print(\"Training took {:.2f}s\".format(t1 - t0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training took 1.19s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMc_-IKPq7LR"
      },
      "source": [
        "Nice! Reducing dimensionality led to over 2Ã— speedup! ðŸ˜Š\n",
        "\n",
        "Let's check the model's accuracy:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFGLA4jVq7LS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce58a641-b5e0-4adb-e53c-31f3f3c72fdf"
      },
      "source": [
        "y_pred = log_clf2.predict(X_test_reduced)\n",
        "accuracy_score(y_test, y_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8396"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-AolAToq7LS"
      },
      "source": [
        "A very slight drop in performance, which might be a reasonable price to pay for a 2Ã— speedup, depending on the application.\n",
        "\n",
        "So there you have it: PCA can give you a formidable speedup...but not always!"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## t-SNE and Isomap"
      ],
      "metadata": {
        "id": "DvDfToF3UGSO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's review these concepts quickly. \n",
        "\n",
        "t-SNE, or t-distributed Stochastic Neighbor Embedding is a technique primarily used to visalize high dimensional data in 2 or 3 dimensions. Conceptually, it tries to preserve clusters in high dimensions as it projects the data into lower dimensions. This is more than a simple strict projection onto an existing axis, which would not preserve the clusters very well as well as a more advanced technique like t-SNE.\n",
        "Pros:\n",
        "- Takes complex, high dimensional data and makes it easy to visualize in 2D or 3D\n",
        "\n",
        "Cons:\n",
        "- Final axes, like PCA, are not interpretable in a standard way, and are a means to an end (visualization) rather than a practical or useful tool in terms of explainability\n",
        "\n",
        "\n",
        "Isomap reduction is a technique based on spectral theory. It is another dimensionality reduction technique that is usually efficient and very generalizable, which will we will compare to t-SNE and PCA.\n",
        "\n",
        "Unlike PCA, t-SNE and Isomap Embeddings are non-linear dimensionality reduction techniques."
      ],
      "metadata": {
        "id": "An1ieGWgUNVK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate and time PCA fit/transform time \n",
        "pca = PCA()\n",
        "t0 = time.time()\n",
        "X_train_reduced = pca.fit_transform(X_train)\n",
        "X_test_reduced = pca.transform(X_test)\n",
        "t1 = time.time()\n",
        "\n",
        "print(f\"PCA took {t1-t0} seconds to fit and transform the data\")\n",
        "\n",
        "# Instantiate and time model\n",
        "model = LogisticRegression(multi_class=\"multinomial\", solver=\"lbfgs\", random_state=42)\n",
        "\n",
        "t2 = time.time()\n",
        "model.fit(X_train_reduced, y_train)\n",
        "t3 = time.time()\n",
        "\n",
        "y_pred = model.predict(X_test_reduced)\n",
        "print(f\"Model built on PCA data took {t3-t2} seconds with accuracy {accuracy_score(y_test, y_pred)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XAzlZmEAULMo",
        "outputId": "73968442-f370-4fb2-d73c-136c0aaec6f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PCA took 1.3701550960540771 seconds to fit and transform the data\n",
            "Model built on PCA data took 3.728423595428467 seconds with accuracy 0.8684\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# Instantiate and time T-SNE fit/transform time \n",
        "tsne = TSNE(learning_rate='auto')\n",
        "t0 = time.time()\n",
        "X = mnist['data'][:7500]\n",
        "X_reduced = tsne.fit_transform(X)\n",
        "X_train_reduced, X_test_reduced = X_reduced[:5000], X_reduced[5000:7500]\n",
        "t1 = time.time()\n",
        "\n",
        "print(f\"T-SNE took {t1-t0} seconds to fit and transform the data\")\n",
        "\n",
        "# Instantiate and time model\n",
        "model = LogisticRegression(multi_class=\"multinomial\", solver=\"lbfgs\", random_state=42)\n",
        "\n",
        "t2 = time.time()\n",
        "model.fit(X_train_reduced, y_train)\n",
        "t3 = time.time()\n",
        "\n",
        "y_pred = model.predict(X_test_reduced)\n",
        "print(f\"Model built on T-SNE data took {t3-t2} seconds with accuracy {accuracy_score(y_test, y_pred)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jklw2mFIRCZG",
        "outputId": "e2091e84-e344-46c9-e117-16ffae0db766"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "T-SNE took 67.84546637535095 seconds to fit and transform the data\n",
            "Model built on T-SNE data took 0.4148986339569092 seconds with accuracy 0.7964\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import Isomap\n",
        "\n",
        "# Instantiate and time Isomap fit/transform time \n",
        "isomap = Isomap(n_jobs=-1, n_neighbors=3)\n",
        "t0 = time.time()\n",
        "X_train_reduced = isomap.fit_transform(X_train)\n",
        "X_test_reduced = isomap.transform(X_test)\n",
        "t1 = time.time()\n",
        "\n",
        "print(f\"Isomap took {t1-t0} seconds to fit and transform the data\")\n",
        "\n",
        "# Instantiate and time model\n",
        "model = LogisticRegression(multi_class=\"multinomial\", solver=\"lbfgs\", random_state=42)\n",
        "\n",
        "t2 = time.time()\n",
        "model.fit(X_train_reduced, y_train)\n",
        "t3 = time.time()\n",
        "\n",
        "y_pred = model.predict(X_test_reduced)\n",
        "print(f\"Model built on Isomap data took {t3-t2} seconds with accuracy {accuracy_score(y_test, y_pred)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0HY1tZ8cROCI",
        "outputId": "49fc5ee8-7e5a-40fa-efed-f9f569d471df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Isomap took 19.256809949874878 seconds to fit and transform the data\n",
            "Model built on Isomap data took 0.5019376277923584 seconds with accuracy 0.5384\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What can we conclude here?\n",
        "\n",
        "In terms of training time: T-SNE > Isomap > PCA\n",
        "\n",
        "In terms of accuracy: PCA > T-SNE > Isomap\n",
        "\n",
        "The accuracy/latency tradeoff is a common one in ML, and your tolerance on either side of that tradeoff depend on the size of data, business requirements, hardware, etc...\n",
        "\n",
        "In this specific case, I would personally choose PCA because I value accuracy over latency unless there is a good reason to do otherwise.\n",
        "\n",
        "Compared to baseline, all methods take longer initially when fitting the dimensionality reduction estimators, but save time later on when transforming new data. Overall, the model will usually have less accuracy since information is lost with dimensionality reduction. In this case, you should experiment with increasing the number of components in T-SNE and Isomap (currently we are using the default of 2). The accuracy will almost certainly be a lot higher.\n",
        "\n",
        "Disclaimer: training and transform time will vary a lot with number of dimensions or components that you choose to keep with dimensionality reduction. The numbers in this example are subject to change depending on those factors, so please keep that in mind when choosing which kind of dimensionality reduction to try when doing this yourself."
      ],
      "metadata": {
        "id": "Yr22NToxJ2ob"
      }
    }
  ]
}